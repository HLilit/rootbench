Formats:
  sqlite, Avro, Parquet, HDF5 row-wise, HD5 column-wise, ProtoBuf, LCIO, Boost serialization?, manual (fread/fwrite)?

ROOT row-wise: split level 0, generate dictionary
single branch deep/flat: not interesting
LZ4
TDataFrame, parallel: wait for new TTreeReaderValue, single threaded should be slower
Large sample: 90, 80, 70... 10 percent branches of all depths, look at H1 Analysis tutorial


Look at:
  file size,
  network latency: fuse interposition, add heavy workload so that latency can hide, Asynchronous prefetch
  running time (warm cache, ssd, hdd)
  memory consumption
  read pattern
  precision
  write performance
  background on library use
  corruption detection

qualitative comparison
  nesting
  column-wise layout
  schema evolution
  platform-independent data types
  compression
  multi-threading / use of multiple threads
  large file support

Notes:
  Generate TTreeReaderValue variables?

HDF5:
  Dataset (directories), data spaces (arrays)
  limited nesting
  file -- memory type mapping manual
  Cumbersome, multi-dimensional arrays and slicing (MPI)
  Compression: only on "closed chunks"

Avro:
  Looks nicely designed
  JSON parsing did not work
  ref counting, easy to make memory leaks
  deflate easy
  reading -- projections

Parquet:
  No documentation, fresh C++ lib ('14)
  Columns accessed by index (error-prone)
  Sizing of row groups manual
  Different compression algorithms
  Lots of dependencies (Thrift, Boost, arrow, brotli)
  LINKING MAKES UNZIPPING TWICE AS SLOW, use -lz -lparquet
  Cumbersome, individual column readers that move independently
  Dremel algorithm for nested columns (?)
  Compression per column
  Checksums

Protobuf:
  Not a file format, struct serialization
  Schema upgrade possible
  CodedInputStream only for <2G files
